from __future__ import annotations

import hashlib
import json
import os
import time
import uuid
from typing import Any, Dict, List, Optional, Tuple

import chromadb
from chromadb.config import Settings
import requests


class VectorMemory:
    """
    Vector memory store using ChromaDB.

    - Embeddings are generated by Ollama (/api/embed) or OpenAI-compatible (/v1/embeddings),
      with legacy fallback (/api/embeddings).
    - Deduplication:
        - exact hash match via metadata filter
        - semantic nearest neighbor distance threshold
    """

    def __init__(
        self,
        persist_dir: str,
        collection_name: str,
        ollama_url: str,
        embed_model: str,
        timeout: float = 120.0,
        dedup_distance: float = 0.08,
    ) -> None:
        self.persist_dir = persist_dir
        self.collection_name = collection_name
        self.ollama_url = ollama_url.rstrip("/")
        self.embed_model = embed_model
        self.timeout = timeout
        self.dedup_distance = dedup_distance

        os.makedirs(self.persist_dir, exist_ok=True)

        self.client = chromadb.PersistentClient(
            path=self.persist_dir,
            settings=Settings(anonymized_telemetry=False),
        )

        # cosine distance works well for embeddings
        self.col = self.client.get_or_create_collection(
            name=self.collection_name,
            metadata={"hnsw:space": "cosine"},
        )

    def _embed(self, text: str) -> Tuple[List[float], float]:
        t0 = time.perf_counter()
        base = self.ollama_url.rstrip("/")

        # Ollama native: POST /api/embed
        r = requests.post(
            f"{base}/api/embed",
            json={"model": self.embed_model, "input": text},
            timeout=self.timeout,
        )
        if r.status_code == 200:
            data = r.json()
            embs = data.get("embeddings")
            if isinstance(embs, list) and embs and isinstance(embs[0], list) and embs[0]:
                return embs[0], (time.perf_counter() - t0)
            raise RuntimeError("Ollama /api/embed: unexpected response (missing embeddings[0])")

        # OpenAI-compatible: POST /v1/embeddings
        r = requests.post(
            f"{base}/v1/embeddings",
            json={"model": self.embed_model, "input": text},
            timeout=self.timeout,
        )
        if r.status_code == 200:
            data = r.json()
            d = data.get("data")
            if isinstance(d, list) and d and isinstance(d[0], dict) and isinstance(d[0].get("embedding"), list):
                return d[0]["embedding"], (time.perf_counter() - t0)
            raise RuntimeError("Ollama /v1/embeddings: unexpected response (missing data[0].embedding)")

        # Legacy: POST /api/embeddings
        r = requests.post(
            f"{base}/api/embeddings",
            json={"model": self.embed_model, "prompt": text},
            timeout=self.timeout,
        )
        r.raise_for_status()
        data = r.json()
        emb = data.get("embedding")
        if not isinstance(emb, list) or not emb:
            raise RuntimeError("Ollama /api/embeddings: empty embedding")
        return emb, (time.perf_counter() - t0)

    @staticmethod
    def _hash_text(text: str) -> str:
        return hashlib.sha256(text.encode("utf-8", errors="ignore")).hexdigest()

    @staticmethod
    def _sanitize_meta(meta: Dict[str, Any]) -> Dict[str, Any]:
        """
        Chroma metadata values should be scalar types.
        """
        safe: Dict[str, Any] = {}
        for k, v in (meta or {}).items():
            if isinstance(v, (str, int, float, bool)) or v is None:
                safe[k] = v
            else:
                safe[k] = json.dumps(v, ensure_ascii=False)
        return safe

    def add(self, text: str, meta: Optional[Dict[str, Any]] = None, memory_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Add a memory item with deduplication.
        Returns:
          { memory_id, skipped, reason, timings: {...}, nearest_distance? }
        """
        timings: Dict[str, float] = {}
        text = (text or "").strip()
        if not text:
            raise ValueError("text is empty")

        h = self._hash_text(text)
        mid = memory_id or str(uuid.uuid4())
        meta = dict(meta or {})
        meta.setdefault("created_at", time.time())
        meta["hash"] = h
        safe_meta = self._sanitize_meta(meta)

        #  Exact hash dedup
        t0 = time.perf_counter()
        # do not include "ids" here; ids are returned separately as "ids" key.
        existing = self.col.get(where={"hash": h}, include=["metadatas", "documents"])
        timings["chroma_get_s"] = time.perf_counter() - t0
        if existing and existing.get("ids"):
            existing_id = existing["ids"][0]
            return {
                "memory_id": existing_id,
                "skipped": True,
                "reason": "exact_hash_duplicate",
                "timings": timings,
            }

        # semantic dedup
        emb, t_emb = self._embed(text)
        timings["ollama_embed_s"] = t_emb

        t1 = time.perf_counter()
        # do not include "ids"
        q = self.col.query(
            query_embeddings=[emb],
            n_results=1,
            include=["distances"],
        )
        timings["chroma_query_s"] = time.perf_counter() - t1

        nearest_dist = None
        if q and q.get("distances") and q["distances"][0]:
            nearest_dist = float(q["distances"][0][0])
            if nearest_dist <= self.dedup_distance:
                existing_id = q["ids"][0][0]
                return {
                    "memory_id": existing_id,
                    "skipped": True,
                    "reason": "semantic_duplicate",
                    "nearest_distance": nearest_dist,
                    "timings": timings,
                }

        t2 = time.perf_counter()
        self.col.add(ids=[mid], documents=[text], embeddings=[emb], metadatas=[safe_meta])
        timings["chroma_add_s"] = time.perf_counter() - t2

        return {
            "memory_id": mid,
            "skipped": False,
            "reason": "",
            "nearest_distance": nearest_dist,
            "timings": timings,
        }

    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        query = (query or "").strip()
        if not query:
            return []

        emb, _t = self._embed(query)

        res = self.col.query(
            query_embeddings=[emb],
            n_results=max(1, int(k)),
            include=["documents", "metadatas", "distances"],
        )
        docs = (res.get("documents") or [[]])[0]
        metas = (res.get("metadatas") or [[]])[0]
        dists = (res.get("distances") or [[]])[0]
        ids = (res.get("ids") or [[]])[0]

        hits: List[Dict[str, Any]] = []
        for mid, doc, meta, dist in zip(ids, docs, metas, dists):
            hits.append({
                "id": mid,
                "text": doc,
                "metadata": meta,
                "distance": float(dist),
            })
        return hits

    def search_with_timings(self, query: str, k: int = 5) -> Tuple[List[Dict[str, Any]], Dict[str, float]]:
        timings: Dict[str, float] = {}
        t0 = time.perf_counter()

        emb, t_emb = self._embed(query)
        timings["ollama_embed_s"] = t_emb

        t1 = time.perf_counter()

        res = self.col.query(
            query_embeddings=[emb],
            n_results=max(1, int(k)),
            include=["documents", "metadatas", "distances"],
        )
        timings["chroma_query_s"] = time.perf_counter() - t1
        timings["vector_search_total_s"] = time.perf_counter() - t0

        docs = (res.get("documents") or [[]])[0]
        metas = (res.get("metadatas") or [[]])[0]
        dists = (res.get("distances") or [[]])[0]
        ids = (res.get("ids") or [[]])[0]

        hits: List[Dict[str, Any]] = []
        for mid, doc, meta, dist in zip(ids, docs, metas, dists):
            hits.append({
                "id": mid,
                "text": doc,
                "metadata": meta,
                "distance": float(dist),
            })
        return hits, timings
